# - 海量日志数据，提取出某日访问百度次数最多的那个IP

题目的隐藏含义就是海量日志数据不能一次读取到内存中，否则直接用HashMap统计IP出现的频率（IP为key，频率为Value），然后按照频率排序就好了。

 这道题的做法是先对文件遍历一遍，将一天访问百度的IP记录到一个单独的文件，如果这个文件还是很大，可以对IP进行hash取余，将IP映射到多个文件中，然后利用HashMap统计每个文件中IP的频率，分别找到每个文件中频率最高的IP，再从这些IP中找到整体频率最高的IP。

 如果是频率最高的前N个怎么办？这时可以在利用HashMap统计每个文件中IP的频率后，维护一个小顶堆，找到TopN。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 N。如果遍历到的IP的出现次数大于堆顶IP的出现次数，则用新IP替换堆顶的IP，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 N 个IP。

# - 寻找热门查询，300万个查询字符串中统计最热门的10个查询

题目详细描述：搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。假设目前有一千万个记录（这些查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个。一个查询串的重复度越高，说明查询它的用户越多，也就是越热门），请你统计最热门的10个查询串，要求使用的内存不能超过1G。 这也就是热榜的简单实现，当然实际的热榜会复杂很多，毕竟还要考虑一部分人要撤热搜或降热度等需求。

**方法一：分治+HashMap** 

还是前面的老方法，先用Hash映射将查询字符串映射到多个小文件中，然后用HashMap统计每个小文件中查询字符串出现的频率（key为热门字符串，value为频率），找到每个小文件中的频率最高的top10，最后通过一个小顶堆统计所有小文件中的top10。

**方法二：直接用HashMap** 

题目中写道，去重后只有300w条数据，每条查询不超过255字节，一共是700多M，小于1G。所以可以直接遍历查询字符串，并存入到HashMap中（key为热门字符串，value为频率），通过小顶堆找到频率最高的top10

 **方法三：前缀树** 

只是把方法二中的HashMap换成了前缀树，在遍历字符串时，在前缀树中查找该字符串，如果找到，则将节点中保存的当前前缀的次数加1，没有找到，则为这个字符串构建新结点，并将新构建的结点中的次数置为1。最后还是通过小顶堆找到频率最高的top10

# - 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词

方法还是前面的分治+HashMap+小顶堆 首先遍历文件，对每个词进行hash，比如hash(x)%5000，将所有词分别存入到5000个小的文件中，每个文件大概200k左右，然后通过HashMap统计每个小文件中词的频率（key为词，value为频率）。对于每个遍历到的词，如果在HashMap中，则将value值加1，不在HashMap中，则将词存入HashMap，并将值置为1。最后构建小顶堆，堆的大小为100，找到频率最高的100个词。

